{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a84d61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "MODEL_PATHS = {\n",
    "    \"svm\": \"model/svm_model.pkl\",\n",
    "    \"scaler\": \"model/length_scaler.pkl\",\n",
    "}\n",
    "DATA_PATH = \"dataset/validation_data.csv\"\n",
    "OUTPUT_PATH = \"5_Results.csv\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d53593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Regex patterns\n",
    "# -------------------------\n",
    "START_PATTERNS = [\n",
    "    r\"^text\\s*\",\n",
    "    r\"^WASHINGTON \\(Reuters\\) -.*? - \",\n",
    "    r\"^LONDON \\(Reuters\\) -.*? - \",\n",
    "]\n",
    "\n",
    "END_PATTERNS = [\n",
    "    r\"text\\s*$\",\n",
    "    r\"-- Source link:.*$\",\n",
    "    r\"ntly produced by the staff of Reuters News Agency\\.*$\",\n",
    "    r\"Featured image.*$\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c97883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Preprocessing\n",
    "# -------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove boilerplate patterns and normalize whitespace.\"\"\"\n",
    "    for pat in START_PATTERNS:\n",
    "        text = re.sub(pat, \"\", text, flags=re.IGNORECASE)\n",
    "    for pat in END_PATTERNS:\n",
    "        text = re.sub(pat, \"\", text, flags=re.IGNORECASE)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Embedding\n",
    "# -------------------------\n",
    "def embed_long_text(\n",
    "    text: str, model: SentenceTransformer, max_tokens: int = 512, stride: int = 256\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Embed long text using chunking and averaging.\"\"\"\n",
    "    tokens = text.split()\n",
    "    embeddings = []\n",
    "\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk_text = \" \".join(tokens[start:end])\n",
    "        embeddings.append(model.encode(chunk_text))\n",
    "        start += stride\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "\n",
    "def embed_corpus(texts: pd.Series, model: SentenceTransformer) -> np.ndarray:\n",
    "    \"\"\"Embed an entire corpus with progress bar.\"\"\"\n",
    "    embeddings = [\n",
    "        embed_long_text(text, model) for text in tqdm(texts, desc=\"Embedding articles\")\n",
    "    ]\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Main pipeline\n",
    "# -------------------------\n",
    "# Load models\n",
    "svm_model = joblib.load(MODEL_PATHS[\"svm\"])\n",
    "length_scaler = joblib.load(MODEL_PATHS[\"scaler\"])\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df_result = df.copy()\n",
    "\n",
    "# Preprocess\n",
    "df[\"combined\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "df[\"combined_clean\"] = df[\"combined\"].apply(clean_text)\n",
    "df[\"combined_len\"] = df[\"combined\"].str.len()\n",
    "\n",
    "# Embed\n",
    "embeddings = embed_corpus(df[\"combined_clean\"], embedder)\n",
    "\n",
    "# Features\n",
    "length_scaled = length_scaler.fit_transform(df[[\"combined_len\"]])\n",
    "X = np.hstack((embeddings, length_scaled))\n",
    "\n",
    "# Predict\n",
    "predictions = svm_model.predict(X)\n",
    "df_result[\"label\"] = predictions\n",
    "\n",
    "# Save\n",
    "df_result.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Results saved to {OUTPUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
